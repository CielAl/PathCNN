{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from utils.dataloader import *\n",
    "#use AUC for AUC and CI, auc2 for precision, AUC and CI, auc3 precision auc and CI\n",
    "from utils.auc import *\n",
    "from utils import new_transforms\n",
    "import argparse\n",
    "import random\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#if opt.experiment is None:\n",
    "#    opt.experiment = 'samples'\n",
    "\n",
    "#os.system('mkdir experiments')\n",
    "#os.system('mkdir {0}'.format(opt.experiment))\n",
    "#os.system('mkdir {0}/images'.format(opt.experiment))\n",
    "#os.system('mkdir {0}/checkpoints'.format(opt.experiment))\n",
    "#os.system('mkdir {0}/outputs'.format(opt.experiment))\n",
    "#opt.manualSeed = random.randint(1, 10000) # fix seed\n",
    "#print(\"Random Seed: \", opt.manualSeed)\n",
    "#random.seed(opt.manualSeed)\n",
    "#torch.manual_seed(opt.manualSeed)\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropout=0.1\n",
    "init='leaky' \n",
    "model_type='alexnet' \n",
    "root_dir='/beegfs/sb3923/DeepCancer/alldata/lung_ds/lung_ds1TilesSorted/' \n",
    "num_class=3 \n",
    "tile_dict_path='/beegfs/sb3923/DeepCancer/alldata/lung_ds/lung_ds1_FileMappingDict.p'\n",
    "batchSize =32\n",
    "opt_init='normal'\n",
    "opt_optimizer= 'Adam'\n",
    "opt_lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Random data augmentation\n",
    "\n",
    "imgSize=299\n",
    "augment = transforms.Compose([new_transforms.Resize((imgSize, imgSize)),\n",
    "                              transforms.RandomHorizontalFlip(),\n",
    "                              new_transforms.RandomRotate(),\n",
    "                              new_transforms.ColorJitter(0.25, 0.25, 0.25, 0.05),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "transform = transforms.Compose([new_transforms.Resize((imgSize,imgSize)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "data = {}\n",
    "loaders = {}\n",
    "\n",
    "for dset_type in ['train', 'valid']:\n",
    "    if dset_type == 'train' :\n",
    "        data[dset_type] = TissueData(root_dir, dset_type, transform = augment)\n",
    "    else:\n",
    "        data[dset_type] = TissueData(root_dir, dset_type, transform = transform)\n",
    "\n",
    "    loaders[dset_type] = torch.utils.data.DataLoader(data[dset_type], batch_size=batchSize, shuffle=True)\n",
    "    print('Finished loading %s dataset: %s samples' % (dset_type, len(data[dset_type])))\n",
    "\n",
    "class_to_idx = data['train'].class_to_idx\n",
    "classes = data['train'].classes\n",
    "\n",
    "print('Class encoding:')\n",
    "print(class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create model objects\n",
    "#model = models.alexnet()\n",
    "model = models.vgg16(num_classes=3)\n",
    "\n",
    "\n",
    "#init_model(model)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(1,20+1):\n",
    "    data_iter = iter(loaders['train'])\n",
    "    i = 0  \n",
    "\n",
    "    while i < len(loaders['train']):\n",
    "        model.train()\n",
    "        img, label = data_iter.next()\n",
    "        i += 1\n",
    "\n",
    "        input_img = Variable(img)\n",
    "        target_label = Variable(label)\n",
    "\n",
    "        train_loss = criterion(model(input_img), target_label)\n",
    "        #print(model(input_img)[0])\n",
    "        # Zero gradients then backward pass\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        ptimizer.step()\n",
    "        \n",
    "        print(train_loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3553ab7a6ee6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load checkpoint models if needed\n",
    "\n",
    "#if opt.cuda:\n",
    "#    model.cuda()\n",
    "\n",
    "# Set up optimizer\n",
    "if opt_optimizer == \"Adam\":\n",
    "    optimizer = optim.Adam(model.parameters(), lr=opt_lr, betas=(0.5, 0.999))\n",
    "elif opt_optimizer == \"RMSprop\":\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr = opt_lr)\n",
    "elif opt_optimizer == \"SGD\": \n",
    "    optimizer = optim.SGD(model.parameters(), lr = opt_lr)\n",
    "else: \n",
    "    raise ValueError('Optimizer not found. Accepted \"Adam\", \"SGD\" or \"RMSprop\"')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_tile_probability(tile_path):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns an array of probabilities for each class given a tile\n",
    "    @param tile_path: Filepath to the tile\n",
    "    @return: A ndarray of class probabilities for that tile\n",
    "    \"\"\"\n",
    "\n",
    "    # Some tiles are empty with no path, return nan\n",
    "    if tile_path == '':\n",
    "        return np.full(num_classes, np.nan)\n",
    "\n",
    "    tile_path = root_dir + tile_path\n",
    "\n",
    "    with open(tile_path, 'rb') as f:\n",
    "        with Image.open(f) as img:\n",
    "            img = img.convert('RGB')\n",
    "\n",
    "    # Model expects a 4D tensor, unsqueeze first dimension\n",
    "    img = transform(img).unsqueeze(0)\n",
    "\n",
    "\n",
    "\n",
    "    # Turn output into probabilities with softmax\n",
    "    var_img = Variable(img, volatile=True)\n",
    "    output = F.softmax(model(var_img)).data.squeeze(0)\n",
    "\n",
    "    return output.cpu().numpy()\n",
    "\n",
    "# Load tile dictionary\n",
    "\n",
    "with open(tile_dict_path, 'rb') as f:\n",
    "    tile_dict = pickle.load(f)\n",
    "\n",
    "def aggregate(file_list, method):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a list of files, return scores for each class according to the\n",
    "    method and labels for those files.\n",
    "    @param file_list: A list of file paths to do predictions on\n",
    "    @param method: 'average' - returns the average probability score across\n",
    "                               all tiles for that file\n",
    "                   'max' - predicts each tile to be the class of the maximum\n",
    "                           score, and returns the proportion of tiles for\n",
    "                           each class\n",
    "    @return: a ndarray of class probabilities for all files in the list\n",
    "             a ndarray of the labels\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    for file in file_list:\n",
    "        tile_paths, label = tile_dict[file]\n",
    "\n",
    "        folder = classes[label]\n",
    "\n",
    "        def add_folder(tile_path):\n",
    "            if tile_path == '':\n",
    "                return ''\n",
    "            else:\n",
    "                return folder + '/' + tile_path\n",
    "\n",
    "        # Add the folder for the class name in front\n",
    "        add_folder_v = np.vectorize(add_folder)\n",
    "        tile_paths = add_folder_v(tile_paths)\n",
    "\n",
    "        # Get the probability array for the file\n",
    "        prob_v = np.vectorize(get_tile_probability, otypes=[np.ndarray])\n",
    "        probabilities = prob_v(tile_paths)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        imgSize = probabilities.shape()\n",
    "        newShape = (imgSize[0], imgSize[1], 3)\n",
    "        probabilities = np.reshape(np.stack(probabilities.flat), newShape)\n",
    "        \"\"\"\n",
    "\n",
    "        if method == 'average':\n",
    "            probabilities = np.stack(probabilities.flat)\n",
    "            prediction = np.nanmean(probabilities, axis = 0)\n",
    "\n",
    "        elif method == 'max':\n",
    "            probabilities = np.stack(probabilities.flat)\n",
    "            probabilities = probabilities[~np.isnan(probabilities).all(axis=1)]\n",
    "            votes = np.nanargmax(probabilities, axis=1)\n",
    "            \n",
    "            out = np.array([sum(votes == i) for i in range(num_classes)])\n",
    "            prediction = out / out.sum()\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Method not valid')\n",
    "\n",
    "        predictions.append(prediction)\n",
    "        true_labels.append(label)\n",
    "\n",
    "    return np.array(predictions), np.array(true_labels)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def early_stop(val_history, t=3, required_progress=0.0001):\n",
    "\n",
    "    \"\"\"\n",
    "    Stop the training if there is no non-trivial progress in k steps\n",
    "    @param val_history: a list contains all the historical validation auc\n",
    "    @param required_progress: the next auc should be higher than the previous by \n",
    "        at least required_progress amount to be non-trivial\n",
    "    @param t: number of training steps \n",
    "    @return: a boolean indicates if the model should early stop\n",
    "    \"\"\"\n",
    "    \n",
    "    if (len(val_history) > t+1):\n",
    "        differences = []\n",
    "        for x in range(1, t+1):\n",
    "            differences.append(val_history[-x]-val_history[-(x+1)])\n",
    "        differences = [y < required_progress for y in differences]\n",
    "        if sum(differences) == t: \n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "stop_training = False\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 3 epochs\n",
    "        Function copied from: https://github.com/pytorch/examples/blob/master/imagenet/main.py\"\"\"\n",
    "    \n",
    "    lr = opt.lr * (0.1 ** (epoch // 3)) # Original\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "\n",
    "\"\"\"\n",
    "Training loop\n",
    "\"\"\"\n",
    "\n",
    "best_AUC = 0.0\n",
    "\n",
    "print('Starting training')\n",
    "start = time.time()\n",
    "#correc=0\n",
    "#total=0\n",
    "\n",
    "print(time.time())\n",
    "for epoch in range(1,20+1):\n",
    "    data_iter = iter(loaders['train'])\n",
    "    i = 0\n",
    "    \n",
    "\n",
    "    while i < len(loaders['train']):\n",
    "        model.train()\n",
    "        img, label = data_iter.next()\n",
    "        i += 1\n",
    "\n",
    "        # Drop the last batch if it's not the same size as the batchsize\n",
    "        if img.size(0) != 32:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "        input_img = Variable(img)\n",
    "        target_label = Variable(label)\n",
    "\n",
    "        train_loss = criterion(model(input_img), target_label)\n",
    "        #print(model(input_img)[0])\n",
    "        # Zero gradients then backward pass\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        print('[%d/%d][%d/%d] Training Loss: %f'\n",
    "               % (epoch, opt.niter, i, len(loaders['train']), train_loss.data[0]))\n",
    "        ii=i+((epoch)*len(loaders['train']))\n",
    "        #get validation AUC every step_freq \n",
    "        if ii % step_freq == 0:\n",
    "            val_predictions, val_labels = aggregate(data['valid'].filenames, method=opt.method)\n",
    "\n",
    "            data_ = np.column_stack((np.asarray(val_predictions),np.asarray(val_labels)))\n",
    "            data_.dump(open('{0}/outputs/val_pred_label_avg_{0}_step_{1}.npy'.format(opt.experiment,str(ii)), 'wb'))\n",
    "            torch.save(model.state_dict(), '{0}/checkpoints/step_{1}.pth'.format(opt.experiment, str(ii)))           \n",
    "            print('validation scores:')\n",
    "\n",
    "            roc_auc = get_auc('{0}/images/val_roc_step_{1}.jpg'.format(opt.experiment,epoch), val_predictions, val_labels, classes = range(num_classes))\n",
    "            for k, v in roc_auc.items(): \n",
    "                if k in range(num_classes):\n",
    "                    k = classes[k] \n",
    "                #experiment.log_metric(\"{0} AUC\".format(k), v)\n",
    "                print('%s AUC: %0.4f' % (k, v))\n",
    "\n",
    "    #save the checkpoint at every epoch\n",
    "    torch.save(model.state_dict(), '{0}/checkpoints/epoch_{1}.pth'.format(opt.experiment, str(epoch)))\n",
    "\n",
    "    #print(time.time())\n",
    "    # Get validation AUC once per epoch\n",
    "    if opt.calc_val_auc:\n",
    "        val_predictions, val_labels = aggregate(data['valid'].filenames, method=opt.method)\n",
    "        data_ = np.column_stack((np.asarray(val_predictions),np.asarray(val_labels)))\n",
    "        data_.dump(open('{0}/outputs/val_pred_label_avg_epoch_{1}.npy'.format(opt.experiment,str(epoch)), 'wb'))\n",
    "\n",
    "        roc_auc = get_auc('{0}/images/val_roc_epoch_{1}.jpg'.format(opt.experiment, epoch),\n",
    "                      val_predictions, val_labels, classes = range(num_classes))\n",
    "\n",
    "        for k, v in roc_auc.items():\n",
    "            if k in range(num_classes):\n",
    "                k = classes[k]\n",
    "\n",
    "            #experiment.log_metric(\"{0} AUC\".format(k), v)\n",
    "            print('%s AUC: %0.4f' % (k, v))\n",
    "\n",
    "    # Stop training if no progress on AUC is being made\n",
    "    if opt.earlystop:\n",
    "        validation_history.append(roc_auc['macro'])\n",
    "        stop_training = early_stop(validation_history)\n",
    "\n",
    "        if stop_training: \n",
    "            print(\"Early stop triggered\")\n",
    "            break\n",
    "\n",
    "# Final evaluation\n",
    "print('Finished training, best AUC: %0.4f' % (best_AUC))\n",
    "end = time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.forward(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
